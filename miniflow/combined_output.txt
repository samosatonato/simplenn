import numpy as np


class Activation:

    _registry = {}

    def __init_subclass__(cls, **kwargs):
        super().__init_subclass__(**kwargs)
        name = getattr(cls, "name", cls.__name__.lower())
        Activation._registry[name] = cls

    @classmethod
    def from_identifier(cls, identifier, **kwargs):
        if isinstance(identifier, cls):
            return identifier
        elif isinstance(identifier, str):
            if identifier in cls._registry:
                return cls._registry[identifier](**kwargs)
            else:
                raise ValueError(f"Unknown identifier: {identifier}")
        else:
            raise TypeError("Expected string identifier or instance of Activation")

    def __init__(self):
        pass

    def __call__(self, x):
        return self._activation(x)
    
    def d(self, x):
        return self._activation_d(x)

    def _activation(self, x):
        raise NotImplementedError
    
    def _activation_d(self, x):
        raise NotImplementedError



class Linear(Activation):
    name = 'linear'

    def _activation(self, x):
        return x

    def _activation_d(self, x):
        return np.ones_like(x)

class Tanh(Activation):
    name = 'tanh'

    def _activation(self, x):
        return np.tanh(x)

    def _activation_d(self, x):
        return 1 - np.tanh(x) ** 2

class Heaviside(Activation):
    name = 'heaviside'

    def _activation(self, x):
        return np.heaviside(x, 0)

    def _activation_d(self, x):
        return np.zeros_like(x)

class ReLU(Activation):
    name = 'relu'

    def _activation(self, x):
        return np.maximum(0, x) 

    def _activation_d(self, x):
        return np.where(x > 0, 1, 0)

class Sigmoid(Activation):
    name = 'sigmoid'

    def _activation(self, x):
        return 1 / (1 + np.exp(-x)) 

    def _activation_d(self, x):
        return x * (1 - x)

class Softmax(Activation):
    name = 'softmax'

    def _activation(self, x):
        exps = np.exp(x - np.max(x, axis=0, keepdims=True))
        return exps / np.sum(exps, axis=0, keepdims=True)

    def _activation_d(self, x):
        raise NotImplementedError('Currently softmax activation supports only output layer.')
    

# TODO

import numpy as np


class Callback:
    pass


class EarlyStopping(Callback):
    def __init__(self, monitor='val_loss', patience=5, min_delta=0, mode='min', baseline=None, restore_best_weights=False):
        super().__init__()
        self.monitor = monitor
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.baseline = baseline
        self.restore_best_weights = restore_best_weights

        self.wait = 0
        self.stopped_epoch = 0
        self.best_score = np.inf if mode == 'min' else -np.inf
        self.best_weights = None
        self.model = None

    def load_model(self, model):
        self.model = model
        self.wait = 0
        self.stopped_epoch = 0
        self.best_score = np.inf if self.mode == 'min' else -np.inf
        self.best_weights = None

    def on_epoch_end(self, epoch, current_score):
        if self.model is None:
             raise RuntimeError("No model loaded.")

        if self.mode == 'min':
            score_check = self.best_score - self.min_delta
            improved = current_score < score_check
        else:
            score_check = self.best_score + self.min_delta
            improved = current_score > score_check

        if self.baseline is not None:
             if self.mode == 'min' and current_score > self.baseline: return False # Not stopping
             if self.mode == 'max' and current_score < self.baseline: return False # Not stopping


        if improved:
            self.best_score = current_score
            self.wait = 0
            if self.restore_best_weights:

                self.best_weights = [np.copy(l.get_weights()) for l in self.model.layers], \
                                   [np.copy(l.get_biases()) for l in self.model.layers]
        else:
            self.wait += 1
            if self.wait >= self.patience:
                self.stopped_epoch = epoch

                print(f"\nEpoch {epoch+1}: Early stopping...")

                if self.restore_best_weights and self.best_weights is not None:

                    print("Restoring model weights from the end of the best epoch.")

                    weights, biases = self.best_weights
                    for i, layer in enumerate(self.model.layers):
                        layer.set_weights(weights[i])
                        layer.set_biases(biases[i])

                return True
            
        return False


class LearningRateScheduler(Callback):
    def __init__(self, schedule_func, optimizer):
        super().__init__()
        
        if schedule_func == 'step':
            self.schedule_func = self.step_decay_schedule
        elif schedule_func == 'exp':
            self.schedule_func = self.exp_decay_schedule
        else:
            raise ValueError('Incorrect decay schedule type.')
        
        self.optimizer = optimizer

    def on_epoch_begin(self, epoch):
        current_lr = self.optimizer.learning_rate
        new_lr = self.schedule_func(epoch, current_lr)
        if new_lr != current_lr:
             print(f"\nEpoch {epoch+1}: LearningRateScheduler setting learning rate to {new_lr}.")
             self.optimizer.learning_rate = new_lr


    def step_decay_schedule(self, epoch, lr):
        initial_lr = 0.01
        drop_rate = 0.5
        epochs_drop = 100
        new_lr = initial_lr * np.pow(drop_rate, np.floor((1+epoch)/epochs_drop))
        return new_lr


    def exp_decay_schedule(self, epoch, lr):
        initial_lr = 0.01
        decay_rate = 0.99
        return initial_lr * (decay_rate ** epoch)


class ModelCheckpoint(Callback):
    pass


import numpy as np
import math


class DataProcessor:

    def __init__(self, preprocessor=None, loader=None):
        default_preprocessor = StandardScaler()
        default_loader = DataLoader()

        self.preprocessor = default_preprocessor or preprocessor
        self.loader = default_loader or loader

    def __call__(self, data):
        raise NotImplementedError('Data processor call not implemented.')

    def load_csv(self, file_path: str, label_col: int = 0, **kwargs) -> tuple[np.ndarray, np.ndarray]:
        kwargs.setdefault('skiprows', 0)
        data = self.loader.load_csv(file_path, **kwargs)

        if data.ndim == 1:
             data = data.reshape(1, -1)
        if label_col < 0:
             label_col = data.shape[1] + label_col

        if not (0 <= label_col < data.shape[1]):
             raise ValueError()

        y = data[:, label_col]
        x = np.delete(data, label_col, axis=1)

        return x, y
    
    def load_dataset(self, file_path: str, label_col: int = 0, skiprows: int = 1, **kwargs) -> tuple[np.ndarray, np.ndarray]:
        data = self.loader.load_dataset(file_path, skiprows=skiprows, **kwargs)

        if data.ndim == 1:
             data = data.reshape(1, -1)
        if label_col < 0:
             label_col = data.shape[1] + label_col

        if not (0 <= label_col < data.shape[1]):
             raise ValueError()

        y = data[:, label_col].astype(kwargs.get('dtype', np.float32))

        x = np.delete(data, label_col, axis=1)

        print(f"Dataset loaded: {x.shape[0]} samples, {x.shape[1]} features, labels shape {y.shape}")
        return x, y


    def batchify(self, x, y, batch_size=1):
        batches = []
        
        for i in range(math.ceil(x.shape[0] / batch_size) - 1):
            batch = (x[i*batch_size : i*batch_size+batch_size], y[i*batch_size : i*batch_size+batch_size])
            batches.append(batch)

        batches.append((x[(math.ceil(x.shape[0] / batch_size) - 1)*batch_size :], y[(math.ceil(x.shape[0] / batch_size) - 1)*batch_size :]))

        return batches

    def shuffle(self, data1=None, data2=None):
        if isinstance(data2, np.ndarray):
            data = np.hstack((data1, data2))
            np.random.shuffle(data)
            return data[:,:data1.shape[1]], data[:,data1.shape[1]:]
        else:
            return np.random.shuffle(data1)

    def encode(self, encoder, y):
        return encoder.encode(y)

    def load_csv(self, file):
        data = self.loader.load_csv(file)
        y = data[:, 0]
        x = data[:, 1:]
        return x, y



class DataPreprocessor:
    
    def __init__(self, encoder=None, scaler=None):

        self.encoder = None
        self.scaler = scaler or StandardScaler()


    def load_encoder(self, encoder):
        if self.encoder is not None:
            raise ValueError('Encoder already loaded.')
        self.encoder = encoder
        return self.encoder
    


class DataLoader:

    """
    Base class for all data loaders.

    ### Shapes:
    - features: (N, input_dim) where N is the number of samples and input_dim is the dimension of features (number of features)
    - labels: (N, output_dim) where N is the number of samples and output_dim is the dimension of labels (number of labels)
    
    - features <=> inputs
    - labels <=> targets
    - features_labels <=> inputs_targets
    """

    def __init__(self):
        self.features_labels = None  # Features and labels
        self.features = None  # Features
        self.labels = None  # Labels


    """
    # Leaving this here for now, maybe will delete later.
    def load_data(self, data):
        if not isinstance(data, np.ndarray):
            raise ValueError('Data must be a numpy array.')
        try:
            if self.data is not None:
                raise ValueError('Data already loaded.')
        except ValueError:
            print('Data already loaded.')
            return self.data
    """            

    def load_csv(self, file_path: str, **kwargs) -> np.ndarray:
        import os
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Data file not found: {file_path}")

        try:
            kwargs.setdefault('delimiter', ',')
            return np.loadtxt(file_path, **kwargs)
        except Exception as e:
            print(f"Error loading data from {file_path}: {e}")
            raise

    def load_dataset(self, file_path: str, skiprows: int = 1, **kwargs) -> np.ndarray:
        kwargs.setdefault('skiprows', skiprows) # Set default skiprows if not passed
        return self.load_csv(file_path, **kwargs)

    def _check_data(self) -> bool:
        if self.features_labels is not None:
            if not isinstance(self.features_labels, np.ndarray):
                print('Error: features_labels is not a numpy array.')
                return False
            if len(self.features_labels.shape) != 2:
                print('Error: features_labels is not a 2D array.')
                return False
            return True
        
        if not isinstance(self.features, np.ndarray):
            print('Error: features is not a numpy array.')
            return False
        if not isinstance(self.labels, np.ndarray):
            print('Error: labels is not a numpy array.')
            return False
        
        if self.features.shape[0] != self.labels.shape[0]:
            print('Error: features and labels have different number of samples.')
            return False

        if self.features.dtype.kind not in ("f", "i"):
            print('Error: features should be numeric.')
            return False
        
        return True

    def load_features_labels(self, features_labels, input_dim=None, output_dim=None):
        if not isinstance(features_labels, np.ndarray):
            raise ValueError('Features and labels must be a numpy array.')
        
        try:
            if self.features is not None or self.labels is not None:
                raise ValueError('Data already loaded.')
        except:
            print('Data already loaded.')
            return self.features_labels

        if input_dim is None or output_dim is None:
            raise ValueError('Input and output dimensions must be specified when loading combined features and labels.')

        if input_dim + output_dim != features_labels.shape[1]:
            raise ValueError('Input and output dimensions do not match the shape of the features and labels array.')

        if self.labels is not None or self.features is not None:
            raise ValueError('Data already loaded.')
        
        return self._split_features_labels(input_dim, output_dim)

    def _split_features_labels(self, input_dim=None, output_dim=None):
        if self.features_labels is not None:
            self.features = self.features_labels[:, :-output_dim]
            self.labels = self.features_labels[:, -output_dim:]

        return self.features, self.labels


    def load_features(self, features):
        if self.features_labels is not None:
            raise ValueError('Data already loaded.')
        self.features = features
        return self.features
    
    def load_labels(self, labels):
        if self.features_labels is not None:
            raise ValueError('Data already loaded.')
        self.labels = labels
        return self.labels
    

    def load_file(self, file_path):

        """
        Load data from a file.
        - data: path to the file
        """

        if not isinstance(file_path, str):
            raise ValueError('File path must be a string.')
        
        try:
            if self.features_labels is not None:
                raise ValueError('Data already loaded.')
        except ValueError:
            print('Data already loaded.')
            return self.features_labels
        
        # Load data from file
        self.features_labels = np.loadtxt(file_path, delimiter=',')

        return self.features_labels
    

class StandardScaler:

    """
    Standardize features by removing the mean and scaling to unit variance.
    """

    def __init__(self):
        self.mean = None
        self.std = None

    def __call__(self, data):
        self.fit(data)
        return self.transform(data)


    def fit(self, data):
        self.mean = np.mean(data, axis=0)
        self.std = np.std(data, axis=0)
        return self.mean, self.std

    def transform(self, data):
        return (data - self.mean) / self.std



class Encoder:
    """
    Base class for all encoders.
    """

    def __init__(self):
        pass

    def __call__(self, data):
        """
        Call the encoder.
        - data: data to be encoded
        """

        if not isinstance(data, np.ndarray):
            raise ValueError('Data must be a numpy array.')
        
        return self.encode(data)
    

    def encode(self, data):
        """
        Encode data.
        """
        raise NotImplementedError('Encoder not implemented.')

    def decode(self, data):
        """
        Decode data.
        """
        raise NotImplementedError('Decoder not implemented.')


# TODO: Add ecncode-decode pa ir-specific object logic.
# TODO: Add string support.
class OneHotEncoder1D(Encoder):

    """
    One-hot encoder for 1D labels.
    - categories: categories to be used for encoding
    - auto: automatically determine categories from the labels
    - labels: labels to be encoded
    """

    def __init__(self, categorization='auto'):
        self.categorization = categorization  # Categorization to be used for encoding
        self.categories = None  # Categories to be used for encoding


    def encode(self, labels):

        """
        Encode labels using one-hot encoding.
        - labels: labels to be encoded
        """

        if not isinstance(labels, np.ndarray):
            raise ValueError('Labels must be a numpy array.')
        
        # if len(labels.shape) != 1:
        #     raise ValueError('Labels must be a 1D array.')
        
        if self.categorization == 'auto':
            # Returns the sorted unique elements of an array.
            categories = np.unique(labels)
        
        label_to_index = {round(label): idx for idx, label in np.ndenumerate(categories)}

        one_hot_labels = np.zeros((len(labels), len(categories)), dtype=int)
        
        for i, label in enumerate(labels):
            col = label_to_index[int(label)]
            one_hot_labels[i, col] = 1
        
        return one_hot_labels

    def decode(self, one_hot_labels):

        """
        Decode one-hot encoded labels.
        - one_hot_labels: one-hot encoded labels to be decoded
        """

        if not isinstance(one_hot_labels, np.ndarray):
            raise ValueError('One-hot labels must be a numpy array.')
        
        if one_hot_labels.shape[1] != len(self.categories):
            raise ValueError('One-hot labels have different number of categories.')
        
        decoded_labels = np.zeros(one_hot_labels.shape[0], dtype=int)
        
        for i in range(one_hot_labels.shape[0]):
            decoded_labels[i] = np.argmax(one_hot_labels[i])
        
        return decoded_labels




def evaluate(model, data):
    activations = model._forwardpass(data)
    print(activations)





from . import utils
from . import activations as ac
import numpy as np


class Layer:

    def __init__(self):
        
        self.initializer = None
        self.w = None
        self.b = None

    def __call__(self, x, ad=None):
        return self._call(x=x, ad=ad)

    def _call(self, x, ad=None):
        raise NotImplementedError

    def build(self, initializer: utils.Initializer = None, wshape=None, bshape=None):
        default_initializer = utils.RandomNormal()
        self.initializer = initializer or default_initializer

        self.add_weights(shape=wshape)
        self.add_biases(shape=bshape)
    
    def add_weights(self, shape=None):
        if self.w is None:
            self.w = self.initializer.new_weights(shape=shape)
        else:
            raise ValueError('Weights already initialized.')

    def add_biases(self, shape=None):
        if self.b is None:
            self.b = self.initializer.new_biases(shape=shape)
        else:
            raise ValueError('Biases already initialized.')

    def get_weights(self):
        return self.w

    def set_weights(self, nw):
        self.w = nw

    def sub_weights(self, dw):
        self.w -= dw

    def get_biases(self):
        return self.b

    def set_biases(self, nb):
        self.b = nb

    def sub_biases(self, db):
        self.b -= db

    def forward(self):
        raise NotImplementedError


class Input:

    def __init__(self, input_dim):
        self.input_dim = input_dim


class SimpleDense(Layer):

    def __init__(self, output_dim, activation, regularizer=None):
        super().__init__()

        self.output_dim = output_dim
        self.regularizer = regularizer

        if isinstance(activation, ac.Activation):
            self.activation = activation
        elif isinstance(activation, str):
            self.activation = ac.Activation.from_identifier(activation)
        else:
            raise ValueError

        self.cache_net = None
        self.cache_a = None
        self.cache_x = None

        self.is_last = False

    def _call(self, x, ad=None):
        return self.forward(x=x, ad=ad)

    def forward(self, x, ad=None):
        
        # Stores the gradient of previous (this input) activation
        self.cache_prev_ad = ad

        # Input: x shape=(N, in_dim)
        # Output: a shape=(N, out_dim)

        # shape=(N, in_dim)
        self.cache_x = None
        # shape=(N, out_dim)
        self.cache_net = None


        if self.activation is None:
            # shape=(N, out_dim)
            self.cache_net = (self.w @ x.T).T
            return self.cache_net, 1
        else:
            # TODO: handle stochastic (one datapoint) input dimensions
            # w: shape=(out_dim, in_dim)
            # xT: shape=(in_dim, N)
            # net' = wxT: shape=(out_dim, N)
            # net = (net')T: shape=(N, out_dim)
            if ad is not None:
                # shape=(N, in_dim)
                self.cache_x = x
                # shape=(N, out_dim)
                self.cache_net = (self.w @ x.T).T + self.b.T
                # shape=(N, out_dim)
                self.cache_a = self.activation(self.cache_net)
                if not self.is_last:
                    return self.cache_a, self.activation.d(self.cache_net)
                else:
                    return self.cache_a, 1       
            else:
                return self.activation((self.w @ x.T).T + self.b.T)

    def backward(self, d):
        return self.gradient(d)

    def gradient(self, delta_net):
        # dc_da: shape=(N, out_dim)
        
        # shape=(out_dim, in_dim)
        dc_dw = (delta_net.T @ self.cache_x) / delta_net.shape[0]

        if self.regularizer:
            dc_dw += self.regularizer.l2_lambda * self.w

        # shape=(1, out_dim)
        dc_db = np.sum(delta_net, axis=0, keepdims=True).T / delta_net.shape[0]

        # shape=(N, in_dim)
        dc_dx = (delta_net @ self.w) * self.cache_prev_ad

        return (dc_dw, dc_db), dc_dx


import numpy as np
from . import activations



class LossFunction:

    """
    Base class for all loss functions.

    - This class computes loss using specific loss function.
    - Also implements unified forward and backward method.

    **Notation**:
    - g: predicted output
    - y: true/target output
    """

    eps = 1e-9

    def __init__(self):
        self.model = None

        self.g = None
        self.y = None

        self.lossval = None  # Loss value
        self.lossgrad = None # Loss gradient
        self.delta_net = None


    def __call__(self, g, y):
        
        """
        Call the loss function.
        """
        self.loss_d(g, y)

        activation = self.model.layers[-1].activation
        if activation is not None:
            if not isinstance(activation, activations.Activation):
                raise ValueError
            self._delta_net(g, y, activation)

        return self.loss(g, y)

    def add_model(self, model):
        self.model = model

    def loss(self, g, y):

        """
        Forward pass of the loss function.
        """
        
        self.lossval = self._loss(g, y)

        return self.lossval

    def _loss(self, g, y):

        """
        Loss function.
        """

        pass

    def loss_d(self, g, y):

        """
        Gradient of the loss function.
        """

        self.lossgrad = self._loss_gradient(g, y)

        return self.lossgrad

    def _loss_gradient(self, g, y):
        
        """
        Derivative of the loss function.
        """

        pass
    
    def _delta_net(self, y, g, activation):
        pass



class MeanSquaredErrorLoss(LossFunction):

    def _loss(self, g, y):
        return np.sum((g - y) ** 2) / g.shape[0]
        
    def _loss_gradient(self, g, y):
        return 2 * (g - y) / g.shape[0]

class CategoricalCrossEntropyLoss(LossFunction):

    def _loss(self, g, y):
        return -1 * np.sum(y * np.log(g + self.eps), axis=1)

    def _loss_gradient(self, g, y):
        return -1 * (y / (g + self.eps))
    
    def _delta_net(self, g, y, activation):
        if isinstance(activation, activations.Softmax):
            self.delta_net = g - y 
        else:
            self.delta_net = self.lossgrad * activation.d(g)

    
class BinaryCrossEntropyLoss(LossFunction):

    def _loss(self, g, y):
        return -1 * np.sum(y * np.log(g + self.eps) + (1 - y) * np.log(1 - g + self.eps))

    def _loss_gradient(self, g, y):
        return -1 * (y / g) + (1 - y) / (1 - g)

class SparseCategoricalCrossEntropyLoss(LossFunction):

    pass


import numpy as np

from . import layers
from . import activations
from . import hypertuner
from . import lossfunctions as lf


class Model:

    def __init__(self, is_eager=False):
        
        self.is_eager = is_eager

        self.is_training = False

    def train(self):
        self.is_training = True

    def untrain(self):
        self.is_training = False

 
class Sequential(Model):

    def __init__(self):
        super().__init__()

        self.layers = []
        self.input_dim = None


    def add(self, layer):
        if isinstance(layer, layers.Input):
            self.input_dim = layer.input_dim
        else:
            if self.is_eager:
                layer.build()
                self.layers.append(layer)
            else:
                self.layers.append(layer)


    def predict(self, x):
        return self._forwardpass(x)

    def _forwardpass(self, x):
        if self.is_training:
            ad = 1
        if isinstance(x, (list)):
            x = np.stack(x)
        for layer in self.layers:
            if self.is_training:
                x, ad = layer(x, ad)
            else:
                x = layer(x)
        return x
    

    def gradients(self, loss_grad):
        return self._backpropagate(loss_grad)

    def _backpropagate(self, delta_net):
        gradients = []
        for i, layer in enumerate(reversed(self.layers)):
            (dw, db), delta_net = layer.backward(delta_net)
            gradients.append((dw, db))
        return gradients

    def sub_weights(self, layers_delta_w):
        for layer, delta_w in zip(self.layers, reversed(layers_delta_w)):
            layer.sub_weights(delta_w)
        return True

    def set_weights(self, w):
        for layer in self.layers:
            layer.set_weights(w)
        return True
    
    def sub_biases(self, layers_delta_b):
        for layer, delta_b in zip(self.layers, reversed(layers_delta_b)):
            layer.sub_biases(delta_b)
        return True

    def set_biases(self, b):
        for layer in self.layers:
            layer.set_biases(b)
        return True

    def build(self, optimizer=None, loss=None, metrics=None):

        self.layers[-1].is_last = True

        if self.is_eager:
            pass
        else:
            wshape = [0, 0]
            bshape = 0
            if self.input_dim:
                wshape[1] = self.input_dim
                bshape = self.input_dim
            self.layers[0].is_first = True
            for layer in self.layers:
                wshape[0] = layer.output_dim
                bshape = layer.output_dim
                layer.build(wshape=wshape, bshape=bshape)
                wshape[1] = wshape[0]

        loss.add_model(self)
        optimizer.add_model(self)


import numpy as np

from . import model as nn


class Optimizer:

    def __init__(self):
        
        self.model = None


    def __call__(self):
        pass

    def add_model(self, model):
        self.model = model
    
    def step(self):

        raise NotImplementedError
    

    def update_weights(self):
        pass

    def update_biases(self):
        pass


class SGD(Optimizer):

    """
    Stochastic Gradient Descent optimizer.
    """

    def __init__(self, learning_rate=0.01, momentum=0.0):
        super().__init__()

        self.learning_rate = learning_rate
        
        self.momentum = momentum
        self.vel_w = None
        self.vel_b = None

        self.gradients = None

    def add_model(self, model):
        super().add_model(model)
    
        if self.momentum > 0:
            self.velocities_w = [np.zeros_like(l.get_weights()) for l in self.model.layers]
            self.velocities_b = [np.zeros_like(l.get_biases()) for l in self.model.layers]

    def step(self, gradients):
        dw_list, db_list = map(list, zip(*gradients))

        for i in range(len(self.model.layers)):
            dw = dw_list[-(i+1)]
            db = db_list[-(i+1)]

            if self.momentum > 0:
                self.velocities_w[i] = self.momentum * self.velocities_w[i] + self.learning_rate * dw
                self.velocities_b[i] = self.momentum * self.velocities_b[i] + self.learning_rate * db
                delta_w = self.velocities_w[i]
                delta_b = self.velocities_b[i]
            else:
                delta_w = self.learning_rate * dw
                delta_b = self.learning_rate * db

            self.model.layers[i].sub_weights(delta_w)
            self.model.layers[i].sub_biases(delta_b)


    def update_weights(self, dw):
        return [w*self.learning_rate for w in dw]
        
    def update_biases(self, db):
        return [b*self.learning_rate for b in db]


class RMSProp(SGD):

    """
    RMSProp optimizer.
    """

    def __init__(self, learning_rate):
        super().__init__(learning_rate)


    def run(self):
        raise NotImplementedError


class Adam(SGD):

    """
    Adam optimizer.
    """

    def __init__(self, learning_rate):
        super().__init__(learning_rate)


    def run(self):
        raise NotImplementedError
    

# TODO

class L2Regularizer:

    def __init__(self, l2_lambda=1):
        self.l2_lambda = l2_lambda
import numpy as np


class Initializer:

    DEFAULT_RANDOM_SEED = 42

    def __init__(self, seed=DEFAULT_RANDOM_SEED):
        self.rng = np.random.default_rng(seed)

    def new_weights(self, shape):
        raise NotImplementedError
    
    def new_biases(self, shape):
        raise NotImplementedError


class RandomNormal(Initializer):

    def new_weights(self, shape):
        return self.rng.normal(size=shape)

    def new_biases(self, shape):
         return np.expand_dims(self.rng.normal(size=shape), axis=1)

class RandomUniform(Initializer):
    def new_weights(self, shape):
        pass
    def new_biases(self, shape):
        pass

# https://cs230.stanford.edu/section/4/#xavier-initialization
class GlorotUniform(Initializer):
    def new_weights(self, shape):
        # shape[0] = output_dim, shape[1] = input_dim
        limit = np.sqrt(6. / (shape[1] + shape[0]))
        return self.rng.uniform(low=-limit, high=limit, size=shape)
    def new_biases(self, shape):
         return np.zeros((shape, 1))

# https://medium.com/the-modern-scientist/exploring-the-he-normal-distribution-in-neural-network-weight-initialization-b802a53074e5
class HeNormal(Initializer):
     def new_weights(self, shape):
         # scale = sqrt(2 / input_dim)
         stddev = np.sqrt(2. / shape[1])
         return self.rng.normal(loc=0.0, scale=stddev, size=shape)
     def new_biases(self, shape):
         return np.zeros((shape, 1))

def split_data_8_2(data):
    samples = np.shape(data)[0]
    tr_data = data[:, :int(samples*0.8)]
    te_data = data[:, int(samples*0.8):]
    return tr_data, te_data


def split_data(data, mode='standard'):
    if mode == 'standard':
        samples = np.shape(data)[0]
        splt = (int(samples*0.8), int(samples*0.15), int(samples*0.05))
        tr_data = data[:, :splt[0]]
        te_data = data[:, splt[0]:splt[1]]
        v_data = data[:, splt[1]:splt[2]]
        return tr_data, te_data, v_data
    else:
        raise ValueError('Invalid split mode.')


def initialize_normal_weights(input_dim, output_dim):

    """
    Initialize weights and biases for a layer with normal distribution.
    """

    W = np.random.randn(output_dim, input_dim) * np.sqrt(2. / input_dim)
    B = np.zeros((output_dim, 1))
    
    return W, B


def initialize_uniform_weights(input_dim, output_dim):

    """
    Initialize weights and biases for a layer with uniform distribution.
    """

    W = np.random.uniform(-1, 1, (output_dim, input_dim))
    B = np.zeros((output_dim, 1))

    return W, B


def initialize_zero_weights(input_dim, output_dim):

    """
    Initialize weights and biases for a layer with zero values.
    """

    W = np.zeros((output_dim, input_dim))
    B = np.zeros((output_dim, 1))

    return W, B



# from ..deprecated.core import MiniFlowWorkbench as Workbench
# from ..deprecated.core import MiniFlowRefinery as Refinery
from . import model
from . import layers
from . import optimizers
from . import lossfunctions
from . import dataprocessor
from . import regularizers
from . import callbacks
